from crewai import Agent, Crew, Process, Task
from crewai_tools import DirectorySearchTool
from dotenv import load_dotenv
import os
from langchain.chains import LLMChain, ConversationalRetrievalChain
from langchain.memory import (
    ConversationBufferMemory,
    ReadOnlySharedMemory,
    ConversationBufferWindowMemory,
)
from langchain.prompts import PromptTemplate
from langchain.agents import Tool
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma

# Load environment variables
load_dotenv()
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# Set API keys
os.environ["GROQ_API_KEY"] = os.getenv("GROQ_API_KEY")
os.environ["HF_TOKEN"] = os.getenv("HF_TOKEN")

from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# Initialize the LLM with the correct API key
llm = ChatOpenAI(
    openai_api_base="https://api.groq.com/openai/v1",
    openai_api_key=os.environ["GROQ_API_KEY"],
    model_name="llama-3.1-70b-versatile",
    temperature=0,
)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2", model_kwargs={"device": "cpu"}
)
# Initialize the search tool with directory search
search_tool = DirectorySearchTool(
    directory="docs-data",
    config=dict(
        llm=dict(
            provider="groq",
            config=dict(
                model="llama-3.1-70b-versatile",
            ),
        ),
        embedder=dict(
            provider="huggingface",
            config=dict(
                model="sentence-transformers/all-MiniLM-L6-v2",
            ),
        ),
    ),
)

""" Retriever for citations """
persist_directory = "db2"
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
retriever = vectordb.as_retriever(search_kwargs={"k": 4})

# Define the Conversational Memory
memory = ConversationBufferWindowMemory(
    memory_key="chat_history", return_source_documents=True, k=5
)
# memory = ConversationBufferMemory(memory_key="chat_history", return_source_documents=True)
readonlymemory = ReadOnlySharedMemory(memory=memory)

# Define a PromptTemplate for summarizing conversations
template = """This is a conversation between a human and AI agent:

{chat_history}

Write a summary of the conversation for {input}:
"""
prompt = PromptTemplate(input_variables=["input", "chat_history"], template=template)

# Define the LLMChain for conversation summarization
summary_chain = LLMChain(
    llm=llm,
    prompt=prompt,
    verbose=True,
    memory=readonlymemory,  # use the read-only memory to prevent the tool from modifying the memory
)

# Combine Tools - Define tool use for search and summarization
tools = [
    Tool(
        name="Search",
        func=lambda query: search_tool.run(query),  # Ensure the tool is callable
        description="Useful for searching documents and answering questions.",
    ),
    Tool(
        name="Summary",
        func=summary_chain.run,
        description="Useful for summarizing a conversation.",
    ),
]

tools1 = [
    Tool(
        name="Summary",
        func=summary_chain.run,
        description="Useful for summarizing a conversation.",
    )
]

# Define the Research Agent with memory enabled and tools integrated
Retriever_Agent = Agent(
    role="Research Agent",
    goal="Search through the directory to find relevant answers.",
    backstory=(
        "You are an assistant for question-answering tasks."
        "Use the information present in the retrieved context to answer the question."
        "You have to provide a clear concise answer."
    ),
    verbose=True,
    allow_delegation=False,
    llm=llm,  # Pass the tools to the agent
    tools=tools,
    # memory=True,  # Enable memory for the agent
)


hallucination_checker = Agent(
    role="Hallucination Checker",
    goal="Check the generated response for hallucinations and ensure factual accuracy.",
    backstory=(
        "You are responsible for reviewing the response generated by the Research Agent. "
        "Your task is to identify and correct any hallucinations or unsupported claims."
    ),
    verbose=True,
    allow_delegation=False,
    llm=llm,
    tools=tools1,
)

content_writer_agent = Agent(
    role="Content Writer",
    goal="Write engaging content based on the provided research or information.",
    backstory=(
        "You are a skilled writer who excels at turning raw data into captivating narratives. "
        "Your task is to write clear, structured, and engaging content."
    ),
    verbose=True,
    allow_delegation=True,
    llm=llm,
    tools=tools1,
    # memory=True,  # Enable memory if the agent needs to remember context across content pieces
)


# Define the retriever task
retriever_task = Task(
    description=(
        "Based on the user's question, extract information for the question {question} "
        "with the help of the tools. Use the Search tool to retrieve information from the directory."
    ),
    expected_output=(
        "You should answer the user's question based on the information retrieved from the directory."
        " Return a clear and concise text as a response. If you don't have the answer, return 'I don't know'."
    ),
    agent=Retriever_Agent,
    # tools=tools,  # Pass the tools to the task as well
    # memory=True,  # Enable memory for the task
)

content_writer_task = Task(
    description=(
        "Use the verified research information provided by the Research Agent. "
        "Your task is to create a well-structured and engaging piece of content. "
        "Focus on clarity, readability, and flow. The content should be suitable for "
        "the intended audience and the topic should be covered comprehensively."
    ),
    expected_output=(
        "A complete and engaging piece of content (e.g., blog post, article, or report) "
        "that is well-structured, easy to read, and aligns with the information provided. "
        "The final content should be formatted and ready for publication."
    ),
    agent=content_writer_agent,
    context=[retriever_task],  # Pass the previous task as context
)

hallucination_task = Task(
    description=(
        "Review the response generated by the Research Agent and check for hallucinations. "
        "Ensure that the response is factually accurate."
    ),
    expected_output=("A validated and corrected response, free of hallucinations."),
    context=[content_writer_task],
    agent=hallucination_checker,
)


# Define the Crew with memory enabled
rag_crew = Crew(
    agents=[Retriever_Agent, content_writer_agent, hallucination_checker],
    tasks=[retriever_task, content_writer_task, hallucination_task],
    verbose=True,  # Enable memory for the entire crew
    process=Process.sequential,
    # memory=True,
    # embedder=dict(
    #         provider="huggingface",
    #         config=dict(
    #             model="sentence-transformers/all-MiniLM-L6-v2",
    #         ),
    #     )
    # process=Process.concurrent
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True
)


def process_llm_response(llm_response):
    print("\nSources:")
    source_temp = []
    for source in llm_response["source_documents"]:
        if source.metadata["source"] not in source_temp:
            source_temp.append(source.metadata["source"])

    for source in source_temp:
        source = (
            source.replace("_", "/")
            .replace("+", ":")
            .replace("docs-data/", "")
            .replace(".txt", "")
        )
        print(source)
    print("\n")
    # print(source.metadata['source'])


# Interactive Loop to handle user queries
while True:
    query = input("Enter a question: ")  # Get the input from the user
    if query.lower() == "exit":  # Exit condition
        break

    inputs = {"question": query}  # Use the input to form the inputs dictionary

    # Call the CrewAI process with the input question
    result = rag_crew.kickoff(inputs=inputs)
    # Print the result
    print(result)
    llm_response = qa_chain.invoke(query)
    process_llm_response(llm_response)
